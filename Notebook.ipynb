{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/Screenshot%202023-05-13%20at%2000.19.54.png)\n",
    "\n",
    "# Your First MLOps Pipeline\n",
    "                         \n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/life-efficient/Your-First-MLOps-Pipeline/blob/main/Notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "## Join the community\n",
    "\n",
    "![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&logo=discord&logoColor=white)\n",
    "\n",
    "## Aim\n",
    "\n",
    "> Build several common components found in MLOps pipelines to illustrate the essential concepts of MLOps.\n",
    "\n",
    "\n",
    "This is by no means a comprehensive overview of all facets of MLOps, but it aims to illustrate the essence of a few of the important concepts.\n",
    "\n",
    "_Note: To run all of the code in this notebook from Colab, you'll need to download the `production_data.py` file [here](https://github.com/life-efficient/Your-First-MLOps-Pipeline), and upload it to Golab.\n",
    "\n",
    "## Outline\n",
    "- What is MLOps?\n",
    "- Turning the ML model training process into a pipeline\n",
    "- Artefact and metadata tracking\n",
    "  - Tensorboard demo\n",
    "- Serving\n",
    "  - APIs\n",
    "- Drift\n",
    "  - Data drift\n",
    "  - Concept drift\n",
    "- Monitoring\n",
    "  - Visualising data drift\n",
    "  - Logging user requests to visualise\n",
    "- Alerting\n",
    "  - Pagerduty example\n",
    "- Retraining\n",
    "  - Collecting more data to train on\n",
    "  - Cron\n",
    "  - Automatic retraining\n",
    "- Automatic deployment (CI/CD)\n",
    "\n",
    "As I go through each section, I'll try to \n",
    "1. Provide an in-notebook implementation to highlight the essence \n",
    "2. Point to further reading about the many MLOps tools available online that will trump our Python implementation.\n",
    "\n",
    "## What is MLOps?\n",
    "\n",
    "- MLOps is shorthand for Machine Learning Operations. \n",
    "- MLOps is everything other than the model training code required to put AI systems into production\n",
    "- MLOps is to AI engineering what devops is to software engineering.\n",
    "- MLOps empowers organisations not just to deploy once, but to deploy over and over again quickly and efficiently by reducing the overhead and increasing automation that usually goes into maintaining machine learning systems in production.\n",
    "\n",
    "## Why do we need MLOps?\n",
    "\n",
    "Automating the deployment process: Deploying machine learning models can be a complex and time-consuming process. MLOps helps automate the deployment process, making it faster and more efficient.\n",
    "\n",
    "Ensuring reproducibility: MLOps provides a framework for versioning, managing and reproducing machine learning models. This ensures that the same results can be obtained in future iterations, even when changes are made to the model.\n",
    "\n",
    "Monitoring and managing models: MLOps enables real-time monitoring of machine learning models, which can help identify and fix issues as they arise. This helps ensure that models remain accurate and up-to-date over time.\n",
    "\n",
    "Scalability: MLOps enables the seamless scaling of machine learning models, which is essential for handling larger datasets or increasing the number of users.\n",
    "\n",
    "Collaboration: MLOps allows for better collaboration between data scientists, engineers, and operations teams, enabling faster development cycles and better integration of machine learning models into existing systems.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "In this example, we're working with data from an online retailer, like Amazon, who make timely offers to their customers after every purchase. We want to build a machine learning model to determine the likelihood of a user claiming the offer so that we can confidently offer it to people who will take it. We don't want to offer it to everyone because it costs us to make the offer, which reduces our margins.\n",
    "\n",
    "- Features:\n",
    "    - `product_rating`: the difference between the average rating for that product and the user's rating\n",
    "    - `delivery_duration`: the difference between the claimed delivery time and the actual delivery time\n",
    "- Label:\n",
    "    - `used_offer`: Whether the user claimed an offer shared with them after their successful delivery\n",
    "\n",
    "However, we know that the distribution of data might change over time:\n",
    "- Buyers may become more or less sensitive to product quality as supply changes\n",
    "- Buyers may become more or less sensitive to delivery duration\n",
    "- Buyers may change whether they claim the offer or not due to changes in the economy\n",
    "\n",
    "> _Note: The datapoints are ordered sequentially in time, so you can see how the data changes over time_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/life-efficient/Your-First-MLOps-Pipeline/main/data/initial_data.csv\")\n",
    "    # ^^^ could read from database, filesystem storage, or other source in another application\n",
    "\n",
    "    features = data.drop(columns=[\"used_offer\"])\n",
    "    labels = data[\"used_offer\"]\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "features, labels = load_data()\n",
    "\n",
    "print(features.describe())\n",
    "print(features.head())\n",
    "print(labels.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What goes into training a machine learning model?\n",
    "\n",
    "You could fill a library with everything that I could mention here, but to summarise it, here are the key steps that go into training a ML model:\n",
    "- Data preparation\n",
    "- Model training\n",
    "- Hyperparameter tuning\n",
    "- Validation set evaluation\n",
    "- Test set evaluation\n",
    "\n",
    "The cells below illustrate what a simple version of this might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(features, labels):\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=42) # split data into train and test data\n",
    "    features_train, features_val, labels_train, labels_val = train_test_split(features_train, labels_train, test_size=0.2, random_state=42) # split train data into train and validation data\n",
    "    return features_train, features_test, features_val, labels_train, labels_test, labels_val\n",
    "\n",
    "features_train, features_test, features_val, labels_train, labels_test, labels_val = split_data(features, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms learning procedure is controlled by some parameters that are set before the learning takes place during what is known as model training. These parameters set before training are called hyperparameters.\n",
    "\n",
    "The cell below defines a function that gets some random hyperparameters. You can tweak these around to try out different model configurations.\n",
    "\n",
    "I won't go into it here, but in practice, you'd want to systematically sample a range of hyperparameters and use the performance on the validation set to determine which performs best on unseen data.\n",
    "\n",
    "We'll use a decision tree classifier as our machine learning model. It's a simple model that can still work remarkably well. You can read about it's hyperparameters [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters():\n",
    "    return {\n",
    "        'max_depth': 4,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "    }\n",
    "\n",
    "hyperparameters = get_hyperparameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_model(features, labels, hyperparameters):\n",
    "    \"\"\"Trains a model on the data\"\"\"\n",
    "\n",
    "    model = DecisionTreeClassifier(\n",
    "        **hyperparameters\n",
    "    )\n",
    "    model.fit(features, labels)\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_model(features_train, labels_train, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement methods for the different parts of the pipeline, including hyperparameter tuning and evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the model on the test set\"\"\"\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(model, features_test, labels_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a function to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import joblib\n",
    "\n",
    "def save_model(model):\n",
    "    \"\"\"Saves the model to disk\"\"\"\n",
    "\n",
    "    joblib.dump(model, \"model.joblib\")\n",
    "\n",
    "save_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting that all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features, labels = load_data()\n",
    "train_features, validation_features, test_features, train_labels, validation_labels, test_labels = split_data(features, labels)\n",
    "hyperparameters = get_hyperparameters()\n",
    "model = train_model(train_features, train_labels, hyperparameters)\n",
    "accuracy = evaluate_model(model, test_features, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining the ML training process\n",
    "\n",
    "Imagine you're a data scientist who's developed a machine learning model. You've found out how to create a model that works by setting the right model configuration and processing the data correctly. However, you know that in the future your data is going to change, because you know that over time, the inputs will change with trends. That means you're going to train this model more than once, which is why it's useful to keep this code for re-use.\n",
    "\n",
    "Now, we can put that all into a function that trains the model from end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(features, labels):\n",
    "    features, labels = load_data()\n",
    "    train_features, validation_features, test_features, train_labels, validation_labels, test_labels = split_data(features, labels)\n",
    "    hyperparameters = get_hyperparameters()\n",
    "    model = train_model(train_features, train_labels, hyperparameters)\n",
    "    accuracy = evaluate_model(model, test_features, test_labels)\n",
    "    return model, accuracy, hyperparameters\n",
    "\n",
    "train_and_evaluate_model(features, labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end Pipeline Tools for Model Training used in the Wild\n",
    "\n",
    "- Airflow\n",
    "- AWS Sagemaker\n",
    "- Databricks\n",
    "- Kubeflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Tracking\n",
    "\n",
    "### What is model metadata?\n",
    "Model metadata refers to information about a machine learning model that is not part of the actual model itself, but rather provides context and descriptive details about the model. This metadata can include information such as the model's name, version, author, creation date, input and output formats, hyperparameters, performance metrics, and more.\n",
    "\n",
    "### Why is it important?\n",
    "Metadata is important because it allows users to understand the characteristics of a model, how it was created, and how it can be used. This information can help users make informed decisions about which models to use for their particular tasks, and can also aid in reproducibility and collaboration between data scientists and other stakeholders.\n",
    "\n",
    "\n",
    "### Implementation\n",
    "The worst way to keep track of your model metadata would be to build your own solution. Everyone's had that idea, and there are simply many way better tools than what you can use off the shelf.\n",
    "\n",
    "Model metadata can be stored and accessed in various ways, such as through a separate file or database that accompanies the model, or as part of the model's documentation or comments within the code itself. Some machine learning frameworks and platforms also provide tools for automatically generating and managing model metadata.\n",
    "\n",
    "In this illustration, I'll use Tensorboard to track model metadata. It's in no way the most sophisticated tool for the job - there are many other more advanced solutions used in the wild that I'll share after the following code. Check out the tensorboard documentation [here](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_hparams).\n",
    "\n",
    "Let's update our training pipeline to log some of the important metadata. Once you run this code, you'll notice a `runs` folder appear, which contains the metadata - it's simply saved in filesystem storage.\n",
    "\n",
    "_Bonus points if you can implement it with a [decorator](https://www.youtube.com/watch?v=fm_oY5tXD_s)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log metadata using tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def log_metadata(model, accuracy, hyperparameters):\n",
    "    \"\"\"Logs metadata to tensorboard\"\"\"\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    writer.add_hparams(\n",
    "        hparam_dict=hyperparameters, \n",
    "        metric_dict={\"accuracy\": accuracy}\n",
    "    )\n",
    "    writer.close()\n",
    "\n",
    "log_metadata(model, accuracy, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Metadata Tracking Tools used in the Wild\n",
    "\n",
    "- Weights and biases\n",
    "- MLFlow\n",
    "- Neptune"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "Now that the model has been trained, the easy part is over. The next step is serving predictions to users. Serving is the essence of what people mean when they talk about \"deployment\".\n",
    "\n",
    "The most common pattern is to serve your model predictions through an API. Users can make requests to the API and receive responses. E.g. Someone can make a request for a prediction, and we serve them that prediction by processing their data with our model.\n",
    "\n",
    "Below is the Python code that defines the API endpoints that could serve the model.\n",
    "\n",
    "_Note: The last line, which sets the API up to listen for requests, won't work in a notebook because it needs to be run from a Python file. However, the methods above will still be defined, so make sure to run this cell because we'll be using those methods going forward._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "\n",
    "model = joblib.load(\"model.joblib\")  # load model from disk\n",
    "\n",
    "api = FastAPI()\n",
    "\n",
    "@api.get(\"/\")\n",
    "def root():\n",
    "    return {\"Hello\": \"World\"}\n",
    "\n",
    "@api.get(\"/predict\") # defines the /predict endpoint\n",
    "def predict(data): # defines that the endpoint takes a parameter called data\n",
    "\n",
    "    data = pd.read_json(data)\n",
    "    prediction = model.predict(data)\n",
    "    return prediction.tolist()\n",
    "\n",
    "\n",
    "# uvicorn.run(api, host='localhost', port=8000) # UNCOMMENT THIS TO RUN IN A PYTHON FILE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the API defined in the above cell, here's what you would do with the code:\n",
    "1. Put into a Python file\n",
    "1. Define a Docker image that contains that file, the model params, and the model class\n",
    "1. Run that Docker container from that image on a computer in the cloud\n",
    "\n",
    "From another file or application, you could make requests to the API like this:\n",
    "\n",
    "_Note: Like the cell above, we expect an error to be thrown, because we can't run an API from a notebook, so the endpoint won't be found when we try to make a request._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_ROOT = \"http://localhost:8000\"\n",
    "API_ENDPOINT = \"/predict\"\n",
    "URL = API_ROOT + API_ENDPOINT\n",
    "\n",
    "# features = pd.DataFrame([\n",
    "#     {\n",
    "#         \"product_rating\": 4.5,\n",
    "#         \"delivery_duration\": 0.2,\n",
    "#     },\n",
    "#     {\n",
    "#         \"product_rating\": 1.5,\n",
    "#         \"delivery_duration\": 0.1,\n",
    "#     },\n",
    "# ])\n",
    "\n",
    "payload = features.to_json()\n",
    "\n",
    "# response = requests.get(URL, data=payload)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the scenario for use in a notebook, I'm just going to call the API method directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = features.to_json()\n",
    "predict(payload)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back two numbers that represent the model's predictions for the two examples we sent with our request."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Clients\n",
    "\n",
    "In our case, here's a simulated example version that works from within a notebook. Often, many APIs, like the OpenAI API for example, come packaged in a Python library. When an API is accessible through a library in a programming language, we call that library a _client_. The client makes the API easy to use because calling the method covers up the direct HTTP request under the hood.\n",
    "\n",
    "_Note: The below code is not something that you would have in a real world situation, it simply poses as an API for us here, because we can't run one through a notebook_\n",
    "\n",
    "In a real situation, an API client's source code might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIClient:\n",
    "    def __init__(self, api_root):\n",
    "        self.api_root = api_root\n",
    "\n",
    "    def predict(self, data):\n",
    "        payload = data.to_json()\n",
    "        response = requests.get(self.api_root + \"/predict\", data=payload)\n",
    "        return response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, that won't work because we can't run an API from within a notebook. So below, I've got another implementation which replaces the request with a direct function call of one of the API's methods. This is NOT something you would see in a real situation.\n",
    "\n",
    "Make sure you understand the difference:\n",
    "- The above code is what a real API client's source code might look like\n",
    "    - It makes a request to an API over the internet\n",
    "- The below code is NOT something you'd see in practice, because the API would be running on a totally separate machine far away in the cloud\n",
    "    - You would not have access to the predict method because it would not be in the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIClient:\n",
    "    def __init__(self, testing=False):\n",
    "        self.model = model\n",
    "        self.step = 0 # adding this step variable so that we can log metadata later in the next section\n",
    "        self.writer = SummaryWriter() # also adding this summary writer here so that we can log metadata later in the next section\n",
    "        if testing:\n",
    "            self.logging_label = \"testing\"\n",
    "        else:\n",
    "            self.logging_label = \"production\"\n",
    "\n",
    "    def predict(self, data):\n",
    "        data = data.to_json()\n",
    "        prediction = predict(data) # IN PRACTICE, THIS LINE WOULD BE A REQUEST TO A REMOTE API (see first APIClient class)\n",
    "        return prediction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows how you would typically use an API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = APIClient(testing=True)\n",
    "api_client.predict(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Tools used in the Wild\n",
    "\n",
    "- FastAPI\n",
    "- AWS Sagemaker\n",
    "- KFServing\n",
    "- Kubernetes\n",
    "- Docker (underpins a lot of the above)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring\n",
    "\n",
    "You can't run ML models in production without keeping an eye on them. We should track some metrics from our API.\n",
    "\n",
    "### Monitoring Tools used in the Wild\n",
    "- Prometheus\n",
    "- Grafana\n",
    "- Evidently\n",
    "- Fiddler\n",
    "- Arize\n",
    "\n",
    "Let's start off by logging some basic hardware metrics, like [CPU utilisation](psutil.cpu_percent(4)) using the Python library `psutil`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the library\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "def log_hardware_metrics():\n",
    "    cpu_utilisation_percent = psutil.cpu_percent(4)\n",
    "    api_client.writer.add_scalar(\n",
    "        \"cpu_utilisation_percent\", \n",
    "        cpu_utilisation_percent, \n",
    "        api_client.step\n",
    "    ) # log the cpu utilisation to tensorboard  \n",
    "    print(f\"CPU utilisation: {cpu_utilisation_percent}%\")\n",
    "\n",
    "    # load1, load5, load15 = psutil.getloadavg() # get the load average over the last 1, 5 and 15 minutes\n",
    "    # cpu_usage = round(100 * load15/os.cpu_count(), 1)\n",
    "    # print(f\"15 minute rolling average CPU utilisation: {cpu_usage}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add that hardware metric logging to our API by adding to the `predict` method that runs when a prediction is requested. This will overwrite the predict method defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):  # defines that the endpoint takes a parameter called data\n",
    "\n",
    "    data = pd.read_json(data)\n",
    "    prediction = model.predict(data)\n",
    "    log_hardware_metrics()\n",
    "    return prediction.tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we request predictions from our API, the API logs the hardware metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = APIClient(testing=True)\n",
    "api_client.predict(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as these hardware metrics though, there are some things that you should track specific to ML. \n",
    "\n",
    "These might include:\n",
    "- Prediction input features\n",
    "- Predictions\n",
    "- Confidence of predictions (for models which product probabalistic predictions)\n",
    "\n",
    "Let's start by logging our model features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "\n",
    "def log_model_inputs(data):\n",
    "    for idx, example in data.iterrows():\n",
    "        api_client.writer.add_scalar(\n",
    "            f\"product_rating/{api_client.logging_label}\",\n",
    "            example[\"product_rating\"], \n",
    "            api_client.step\n",
    "        )\n",
    "        api_client.writer.add_scalar(\n",
    "            f\"delivery_duration/{api_client.logging_label}\",\n",
    "            example[\"delivery_duration\"], \n",
    "            api_client.step\n",
    "        )\n",
    "        api_client.step += 1\n",
    "    # print(\"Logging inputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = APIClient(testing=True)\n",
    "\n",
    "def predict(data):  # defines that the endpoint takes a parameter called data\n",
    "    data = pd.read_json(data)\n",
    "    log_model_inputs(data) ## add input logging\n",
    "    prediction = model.predict(data)\n",
    "    # log_hardware_metrics()\n",
    "    return prediction.tolist()\n",
    "\n",
    "\n",
    "api_client.predict(features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the entire training dataset through this logging process and see how the results look.\n",
    "\n",
    "_Note: We haven't set a `global_step` parameter in our `add_scalar` function calls, so make sure to view the graphs on \"wall time\" not \"step\", because they will all have the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter # this is just here so that the \"Launch Tensorboard Session\" shows up in VSCode\n",
    "\n",
    "api_client.predict(features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/training_data_drift.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift - Is my problem changing over time?\n",
    "\n",
    "> _Drift_ is when data changes over time. \n",
    "\n",
    "There are two main types of drift:\n",
    "- Data drift\n",
    "- Concept drift\n",
    "\n",
    "### Data Drift\n",
    "\n",
    "Data drift refers to a change in the statistical properties of the input data over time. This can occur for a variety of reasons, such as changes in the data collection process, changes in the user population, or changes in the underlying distribution of the data. Data drift can lead to a decrease in model accuracy if the model is not updated to reflect the new data distribution.\n",
    "\n",
    "For example, if a model is trained to classify images of cats and dogs based on certain visual features, and the distribution of images in the real-world data changes over time (e.g., more pictures of certain breeds), this can lead to data drift and cause the model to make more errors in its predictions.\n",
    "\n",
    "### Concept Drift\n",
    "\n",
    "Concept drift, on the other hand, refers to a change in the relationship between the input features and the target variable over time. This can occur when the underlying relationships between the features and target variable change due to external factors, such as changes in customer behavior or market trends. Concept drift can lead to a decrease in model accuracy if the model is not updated to reflect the new relationships between the features and target variable.\n",
    "\n",
    "For example, if a model is trained to predict customer churn based on certain customer attributes (e.g., age, income, location), and the factors that influence customer churn change over time (e.g., new competitors entering the market), this can lead to concept drift and cause the model to make more errors in its predictions.\n",
    "\n",
    "### Visualising data drift\n",
    "\n",
    "Below, I use a class `RealWorldDataStream` that I've implemented to simulate a sequence of data being sent from real customers to your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from production_data import RealWorldDataStream\n",
    "\n",
    "api_client = APIClient(testing=False)\n",
    "\n",
    "for idx, datapoint in enumerate(RealWorldDataStream()):\n",
    "    api_client.predict(datapoint) # send request to API and log metrics of production data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Build a drift detection system that prints when drift is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from production_data import RealWorldDataStream\n",
    "\n",
    "class DriftDetector:\n",
    "    \"\"\"A class used to monitor a single value of a datapoint for drift\"\"\"\n",
    "\n",
    "    def __init__(self, metric_name):\n",
    "        self.metric_name = metric_name  # name of the dictionary column you want from the data\n",
    "        self.mean = None\n",
    "        self.examples_seen = 0\n",
    "        self.alpha = 0.9\n",
    "        self.threshold = 1\n",
    "\n",
    "    def compute_running_mean(self, data):\n",
    "        \"\"\"Computes the running mean of the input data for each element provided in the array\"\"\"\n",
    "        if self.examples_seen == 0:\n",
    "            self.mean = data\n",
    "        else:\n",
    "            self.mean = (self.mean * self.examples_seen +\n",
    "                         data) / (self.examples_seen + 1)\n",
    "            self.exponential_rolling_mean = self.mean * \\\n",
    "                self.alpha + data * (1 - self.alpha)\n",
    "        self.examples_seen += 1\n",
    "\n",
    "    def monitor(self, data):\n",
    "        \"\"\"Monitors the input data for drift\"\"\"\n",
    "\n",
    "        data = data[self.metric_name].to_numpy()\n",
    "        print(data)\n",
    "\n",
    "        # print(\"Monitoring data\")\n",
    "        # print(self.mean)\n",
    "        self.compute_running_mean(data)\n",
    "        # print(\"New mean\", self.mean)\n",
    "\n",
    "        if self.examples_seen == 1:  # if the first example is seen\n",
    "            self.mean_upper_threshold = self.mean + self.threshold\n",
    "            self.mean_lower_threshold = self.mean - self.threshold\n",
    "\n",
    "        if self.drift_detected():\n",
    "            pass\n",
    "            # print(\"Retraining model\")\n",
    "\n",
    "    def drift_detected(self):\n",
    "        \"\"\"Detects drift in the monitored data\"\"\"\n",
    "        drift_detected = False\n",
    "        if self.mean > self.mean_upper_threshold or self.mean < self.mean_lower_threshold:\n",
    "            print(\n",
    "                f\"Drift detected in {self.metric_name} (rolling average ({self.mean}) exceeded threshold range ({self.mean_lower_threshold}, {self.mean_upper_threshold}))\")\n",
    "            drift_detected = True\n",
    "        return drift_detected\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = RealWorldDataStream()\n",
    "    feature_1_drift_detector = DriftDetector(\"product_rating\")\n",
    "    feature_2_drift_detector = DriftDetector(\"delivery_duration\")\n",
    "    # label_drift_detector = DriftDetector(\"label\") # we don't have the label... but if you did\n",
    "    for example in dataset:\n",
    "        feature_1_drift_detector.monitor(example)\n",
    "        feature_2_drift_detector.monitor(example)\n",
    "        # label_drift_detector.monitor(label)\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alerting\n",
    "\n",
    "> Without alerting, all the monitoring in the world might not make you aware of the problem.\n",
    "\n",
    "Detecting and responding to data drift and other ML-related issues is critical to maintaining the accuracy, fairness, and performance of a machine learning model in production. By setting up alerts and automating the monitoring process, you can ensure that issues are detected early and addressed in a timely manner.\n",
    "\n",
    "One typical practice is for MLOps engineers to take shifts where they are on call, prepared and ready to look into and address any issues that come up. \n",
    "\n",
    "One industry-grade tool that I like to use is PagerDuty. It's a tool that can trigger phone calls, texts, and push notification alerts to members on your team. \n",
    "\n",
    "You can create an account [here](https://www.pagerduty.com/).\n",
    "\n",
    "I once heard people talking about \"carrying the pager\", and made the mistake of thinking that someone had an old-school, physical pager. These days, alerts come straight to your phone.\n",
    "\n",
    "Check out the pagerduty Python client documentation [here](https://pagerduty.github.io/pdpyras/user_guide.html#:~:text=Events%20API%20v2-,%C2%B6,-Trigger%20and%20resolve).\n",
    "\n",
    "\n",
    "Setup\n",
    "- To trigger an event, you will need to set up a service (e.g. \"Production AI Monitoring\" service).\n",
    "- To authenticate with the API you will need a _routing key_\n",
    "    - It acts as your secret password\n",
    "    - It's much like a regular API key\n",
    "    - Once you've set up a service, you can find the integration key by clicking the cog next to the name of the integration as shown in the diagram below\n",
    "\n",
    "![](./images/integration-key.png)\n",
    "\n",
    "\n",
    "_Note: PagerDuty is just an API like the one we developed above. The Python client sends requests to the PagerDuty API, and in response you get a phone call. APIs can do whatever you program them to, and many companies are just a huge scale API._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdpyras\n",
    "\n",
    "API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "API_KEY = \"R03DTI65M2VYDU5BJOG73SVI5BUVLKAV\"\n",
    "\n",
    "\n",
    "events_session = pdpyras.EventsAPISession(API_KEY)\n",
    "\n",
    "dedup_key = events_session.trigger(\"Data drift detected\", 'dusty.old.server.net')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining\n",
    "\n",
    "Automatic retraining of machine learning models is a technique used to improve the performance of a model over time by continuously updating it with new data. This approach is particularly useful when the underlying data is changing or evolving, such as in the case of sensor data, social media data, or financial data.\n",
    "\n",
    "### Challenge: Implement automatic retraining and redeployment of the model when significant data drift is detected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
